<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    
    <title>【传媒聚焦】实验室CVPR论文成果获《机器之心》报道：CVPR 2017 国内外亮点论文汇集 | 中国科学院大学·模式识别与智能系统开发实验室</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content="" />
    
    <meta name="description" content="媒体报道：电子学院模式识别与智能系统开发实验室叶齐祥教授、博士生柯炜、硕士生周彦钊撰写的三篇论文日前被“2017年度计算机视觉与模式识别国际会议”（2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017）录用，其中柯炜的论文以口头报告（Oral）的形式接收。    今年该实验室共投稿论文4篇，中稿3篇，中">
<meta property="og:type" content="article">
<meta property="og:title" content="【传媒聚焦】实验室CVPR论文成果获《机器之心》报道：CVPR 2017 国内外亮点论文汇集">
<meta property="og:url" content="http://ucassdl.cn/content/news/2017-7-23.html">
<meta property="og:site_name" content="中国科学院大学·模式识别与智能系统开发实验室">
<meta property="og:description" content="媒体报道：电子学院模式识别与智能系统开发实验室叶齐祥教授、博士生柯炜、硕士生周彦钊撰写的三篇论文日前被“2017年度计算机视觉与模式识别国际会议”（2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017）录用，其中柯炜的论文以口头报告（Oral）的形式接收。    今年该实验室共投稿论文4篇，中稿3篇，中">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://ucassdl.cn/images/news/2017-7-23_0.jpg">
<meta property="og:updated_time" content="2018-04-04T09:21:36.447Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【传媒聚焦】实验室CVPR论文成果获《机器之心》报道：CVPR 2017 国内外亮点论文汇集">
<meta name="twitter:description" content="媒体报道：电子学院模式识别与智能系统开发实验室叶齐祥教授、博士生柯炜、硕士生周彦钊撰写的三篇论文日前被“2017年度计算机视觉与模式识别国际会议”（2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017）录用，其中柯炜的论文以口头报告（Oral）的形式接收。    今年该实验室共投稿论文4篇，中稿3篇，中">
<meta name="twitter:image" content="http://ucassdl.cn/images/news/2017-7-23_0.jpg">
    

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/2.0.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    


</head>

<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="http://ucassdl.cn" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">模式识别与智能系统开发实验室</p>
                            <p class="subtitle">Pattern Recognition and Intelligent System Development Laboratory</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">主 页</a>
                                </li>
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/categories/%E6%96%B0%E9%97%BB/">动 态</a>
                                </li>
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/categories/%E5%9B%A2%E9%98%9F/">团 队</a>
                                </li>
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/categories/%E6%88%90%E6%9E%9C/">成 果</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="搜索" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="想要查找什么..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>
        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/新闻/">新闻</a>
    </h1>
</div>
                        <div class="main-body-content">
                            <article id="post-news/2017-7-23" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        【传媒聚焦】实验室CVPR论文成果获《机器之心》报道：CVPR 2017 国内外亮点论文汇集
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/content/news/2017-7-23.html" class="article-date">
            <time datetime="2017-07-22T16:00:00.000Z" itemprop="datePublished">2017-07-23</time>
        </a>
    </div>

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <table class="legacy" style="width:100%; text-align:center; margin:0; padding:0;"><tbody><tr><td><p></p><div class="WordSection1" style="layout-grid:15.6pt"><p style="
    font-size: 16px;
          "><a href="#PriSDL" style="
    font-size: 17px;
    text-decoration: underline;
">媒体报道</a>：电子学院模式识别与智能系统开发实验室叶齐祥教授、博士生柯炜、硕士生周彦钊撰写的三篇论文日前被“2017年度计算机视觉与模式识别国际会议”（<strong>2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017）</strong>录用，其中柯炜的论文以<strong>口头报告（Oral）</strong>的形式接收。<br>    今年该实验室共投稿论文4篇，中稿3篇，中稿篇数之多和中稿比例之高在国内研究机构尚不多见。其中，最年轻的作者周彦钊同学目前是<strong>二年级硕士</strong>研究生。</p><hr><p style="text-align: center; line-height: 21pt; background: white none repeat scroll 0% 0%;"><span style="font-family: 宋体; background: #757576 none repeat scroll 0% 0%; color: white; font-size: 12pt;">机器之心原创</span></p><p style="text-align: center; line-height: 21pt; background: white none repeat scroll 0% 0%;"><b><span style="font-family: 宋体; color: #7f7f7f; font-size: 9pt;">参与：李亚洲、路雪、李泽南</span></b></p><p style="margin:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:
inter-ideograph;text-indent:18.0pt;line-height:21.0pt;background:white"><em><span style="font-size:9.0pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#888888">深度学习界的「春晚」<span lang="EN-US">CVPR 2017 </span>已在夏威夷火奴鲁鲁<span lang="EN-US"> Hawaii Convention Center<br></span>开幕，在本次大会接收的众多论文当中，有华人参与的接近半数。这七百余篇论文中有哪些亮点？众多参会的中国研究机构又贡献了多少？我们为你整理了一篇观看指南。</span></em></p><img alt="会议地点" src="/images/news/2017-7-23_0.jpg" style="
    max-width: 100%;
"><p style="margin:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:
inter-ideograph;text-indent:24.0pt;line-height:21.0pt;background:white"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">不久之前，谷歌发布了<span lang="EN-US"> 2017 </span>版学术指标。从这次公布的数据来看，只有少数学术会议的影响因子超过了热门的预印版论文发布平台<span lang="EN-US"> arXiv</span>。</span></p><img alt="arXiv" src="/images/news/2017-7-23_1.jpg" style="
    max-width: 100%;
"><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">其中可以看到，在计算机视觉与模式识别领域，<span lang="EN-US">CVPR </span>是影响力最大的论文发布平台。<span lang="EN-US">CVPR </span>全称为「<span lang="EN-US">IEEE Conference on Computer Vision<br>and Pattern Recognition</span>」（计算机视觉与模式识别会议），是近年来计算机视觉领域全球最影响力、内容最全面的顶级学术会议，由专业技术学会<span lang="EN-US"> IEEE</span>（电气和电子工程师协会）主办。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">不仅在学术领域，随着深度学习在图像处理领域的应用热潮，越来越多的业界研究机构也在将目光投向<span lang="EN-US"><br>CVPR</span>，从数量众多的大会赞助商中，我们就可以感受到这一活动的关注度之高。在漫长的等待之后，本届大会已于<span lang="EN-US"><br>2017 </span>于<span lang="EN-US"> 7 </span>月<span lang="EN-US"> 21 </span>日在美国夏威夷开幕，并将举行至<span lang="EN-US"> 7 </span>月<span lang="EN-US"> 26 </span>日。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">作为领域内具有权威性的会议，今年的<span lang="EN-US"> CVPR </span>共收到有效提交论文<span lang="EN-US"> 2680 </span>篇，其中<span lang="EN-US"> 2620 </span>篇论文经过完整评议，最终总计<span lang="EN-US"> 783 </span>篇被正式录取（占总提交数的<span lang="EN-US"> 29%</span>）。被接收的论文中，<span lang="EN-US">71 </span>篇将进行长口头演讲，<span lang="EN-US">144 </span>篇进行短亮点演讲。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">华人不仅占据了论文作者的半壁江山，国内众多人工智能公司也已摩拳擦掌要在<span lang="EN-US"> CVPR </span>大会上大显身手了。本文中，我们将盘点国内外人工智能公司在<span lang="EN-US"> CVPR 2017 </span>上展现的技术，以及即将举办的精彩活动。文后我们附上了机器之心此前报道过的<span lang="EN-US"> CVPR 2017 </span>论文。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p align="center" style="margin:0cm;margin-bottom:.0001pt;text-align:center;
line-height:21.0pt;background:white;max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important;min-height: 1em;orphans: auto;text-align:start;
widows: 1;-webkit-text-stroke-width: 0px;word-spacing:0px"><strong><span style="font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">产业界<span lang="EN-US"> CVPR<br>2017 </span>论文</span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">有众多业界公司参与了<span lang="EN-US"> CVPR 2017</span>。据大会官网的数据统计，全球共有约<span lang="EN-US"> 90 </span>家企业参与到本次大会中。海外科技巨头谷歌、微软、<span lang="EN-US">Facebook</span>、亚马逊、苹果、英特尔、英伟达等，毫无意外都有论文被接收。即使是最为低调的苹果，在日前新开的在线期刊《<span lang="EN-US">Apple Machine Learning Journal</span>》中，最先介绍的也是该公司将在<span lang="EN-US">&nbsp;CVPR 2017 </span>大会上展示的论文。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">把目光转向国内公司，腾讯、阿里巴巴、京东、滴滴等大型互联网公司，和商汤、<span lang="EN-US">Momenta</span>、驭势、格灵深瞳等初创企业也都或多或少地参与了这次大会。国内公司不仅提交论文，也在会议期间举办演讲和各类活动，中国公司在机器学习领域的积累正在逐渐显现。</span></span></p><img alt="投资商" src="/images/news/2017-7-23_2.jpg" style="
    max-width: 100%;
"><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">以下，我们将介绍国内几家人工智能公司和机构在<span lang="EN-US"> CVPR 2017 </span>上的论文接收情况，其中也包括这些参会者在<span lang="EN-US"> CVPR 2017 </span>上将要介绍的重点论文。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">微软亚洲研究院</span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">据机器之心了解，微软有<span lang="EN-US"> 28 </span>篇论文被<span lang="EN-US"><br>CVPR 2017 </span>接收，其中微软亚洲研究院有<span lang="EN-US"> 18 </span>篇论文被接收。此外，微软全球执行副总裁沈向洋将在大会上发表主旨演讲。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">在<span lang="EN-US"> 6 </span>月<span lang="EN-US"> 16 </span>日北京中关村微软大厦举办的「微软亚洲研究院创研论坛——<span lang="EN-US">CVPR 2017 </span>论文分享会」上，微软亚研的研究员已经分享了数篇被<span lang="EN-US"> CVPR 2017<br></span>接收的论文：</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">论文：<span lang="EN-US">StyleBank: An Explicit Representation for Neural Image Style Transfer</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：作者们在这一设计中运用卷积神经网络作为基础，在通过卷积作用得到特征层后，加入风格化分支——<span lang="EN-US">StyleBank </span>层作相应处理，可以得到很好的图像效果。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">论文：<span lang="EN-US">Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：微软亚洲研究院姚霆的相关研究成果为<span lang="EN-US"> Image Captioning with Attributes</span>，通过利用属性（<span lang="EN-US">attributes</span>）对图像标注进行改进，这种方法有很大的潜力可以成为生成开放性词汇句子（<span lang="EN-US">open-vocabulary sentences</span>）的有效方法。这种以搜索为基础的方法，应用卷积神经网络加循环神经网络，可以使图像标注系统更加实用。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">论文：<span lang="EN-US">Collaborative Deep Reinforcement Learning for Joint Object Search</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：作者们提出了一种新的多智能体间的<span lang="EN-US"> Q-</span>学习的方法，即门控选通式连接（<span lang="EN-US">gated cross connections</span>）的深度<span lang="EN-US"> Q </span>网络——给交流模块设计一个门控结构，可以让每个智能体去选择相信自己还是相信别人。这是一种对虚拟智能体进行联合训练的高效方法。它有效地利用了相关物体间的有用的上下文情境信息（<span lang="EN-US">contextual information</span>），并且改进了目前最先进的主动定位模型（<span lang="EN-US">active localization models</span>）。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">论文：<span lang="EN-US">Neural Aggregation Network For Video Face Recognition</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：传统的深度学习方法在进行人脸识别时需要对每一帧都进行特征提取，这样的效率是很低的。而<span lang="EN-US"><br>NAN </span>网络可以对视频或目标对象进行高度紧凑（<span lang="EN-US">highly-compact</span>）的表征（<span lang="EN-US">128-d</span>）；利用注意力机制（<span lang="EN-US">attention mechanism</span>）进行学习型聚合（<span lang="EN-US">learning-based aggregation</span>）；这种网络在三个关于人脸视频的基准中都有着一流的表现。作者认为这种聚合网络是简单并且通用的，今后也会用于其它一些视频识别的任务当中。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">在微软众多被接收的论文中，我们注意到一篇<span lang="EN-US"> Oral </span>论文：<span lang="EN-US">Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition</span>，介绍了一种端到端，逐层集中<span lang="EN-US"> Attention </span>到有用位置的方法。该研究作者为微软亚研研究员梅涛等人。大会期间，该团队将上台进行<span lang="EN-US"> 12 </span>分钟的演讲，详细介绍此项研究成果。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">阿里巴巴</span></span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">据机器之心了解，阿里巴巴共有四篇论文被<span lang="EN-US"> CVPR 2017<br></span>接收，其中阿里<span lang="EN-US"> AI Lab 3 </span>篇，阿里<span lang="EN-US"> iDST 1 </span>篇；阿里<span lang="EN-US"> iDST </span>视觉计算负责人华先胜华先胜也将在<span lang="EN-US"> ReID &amp; MTMCT<br>Workshop </span>上发表题为《<span lang="EN-US">Practices of Large-Scale Target<br>Re-Identification</span>》的演讲。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">以下为阿里巴巴<span lang="EN-US"> CVPR 2017 </span>论文：</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">Deep Level Sets for Salient Object Detection</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Ping Hu</span>、<span lang="EN-US">Bing Shuai</span>、<span lang="EN-US">Jun Liu</span>、<span lang="EN-US">Gang Wang</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">Global Context-Aware Attention LSTM Networks for 3D Action Recognition</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Jun Liu</span>、<span lang="EN-US">Gang Wang</span>、<span lang="EN-US">Ping Hu</span>、<span lang="EN-US">Ling-Yu Duan</span>、<span lang="EN-US">Alex C. Kot</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">Episodic CAMN: Contextual Attention-based Memory Networks With Iterative Feedback For Scene Labeling</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Abrar H. Abdulnabi, Bing Shuai, Stefan Winkler, Gang Wang</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">Video to Shop: Exactly Matching Clothes in Videos to Online Shopping Images</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Zhi-Qi Cheng</span>、<span lang="EN-US">Xiao Wu</span>、<span lang="EN-US">Yang Liu</span>、华先胜</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">此外，阿里在<span lang="EN-US"> CVPR </span>期间也将会演示拍立淘技术，简单介绍就是拍照搜索技术。</span></span></p><img alt="拍立淘" src="/images/news/2017-7-23_3.jpg" style="
    max-width: 100%;
"><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong><span lang="EN-US" style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">&nbsp;</span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white"><strong><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">腾讯<span lang="EN-US"> AI Lab</span></span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作为腾讯最新成立的人工智能实验室，腾讯<span lang="EN-US"> AI Lab </span>在基础层的技术研究上实力非常。机器之心是首家报道腾讯<span lang="EN-US"> AI Lab </span>研究的媒体，我们注意到腾讯<span lang="EN-US"> AI Lab </span>首次公开的有关风格迁移的研究论文此次也被<span lang="EN-US"> CVPR 2017 </span>所接收。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">据统计，腾讯<span lang="EN-US"> AI Lab </span>共有<span lang="EN-US"> 6 </span>篇论文被<span lang="EN-US"> CVPR 2017 </span>接收，以下是对这些论文的简要介绍：</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文一：<span lang="EN-US">Real Time Neural Style Transfer for Videos</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：本文用深度前向卷积神经网络探索视频艺术风格的快速迁移，提出了一种全新两帧协同训练机制，能保持视频时域一致性并消除闪烁跳动瑕疵，确保视频风格迁移实时、高质、高效完成。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文二：<span lang="EN-US">WSISA: Making Survival Prediction from Whole Slide Histopathological Images</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">简介：论文首次提出一种全尺寸、无标注、基于病理图片的病人生存有效预测方法<span lang="EN-US"> WSISA</span>，在肺癌和脑癌两类癌症的三个不同数据库上性能均超出基于小块图像方法，有力支持大数据时代的精准个性化医疗。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文三：<span lang="EN-US">SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：针对图像描述生成任务，<span lang="EN-US">SCA-CNN </span>基于卷积网络的多层特征来动态生成文本描述，进而建模文本生成过程中空间及通道上的注意力模型。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文四：<span lang="EN-US">Deep Self-Taught Learning<br>for Weakly Supervised Object Localization</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：本文提出依靠检测器自身不断改进训练样本质量，不断增强检测器性能的一种全新方法，破解弱监督目标检测问题中训练样本质量低的瓶颈。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文五：<span lang="EN-US">Diverse Image Annotation</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：本文提出了一种新的自动图像标注目标，即用少量多样性标签表达尽量多的图像信息，该目标充分利用标签之间的语义关系，使得自动标注结果与人类标注更加接近。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文六：<span lang="EN-US">Exploiting Symmetry and/or Manhattan Properties for 3D Object Structure Estimation from Single and<br>Multiple Images</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：基于曼哈顿结构与对称信息，文中提出了单张图像三维重建及多张图像<span lang="EN-US"> Structure from Motion </span>三维重建的新方法。</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">商汤科技</span></span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作为一家专注于计算机视觉和深度学习的创业公司，商汤科技也将在<span lang="EN-US"> CVPR 2017 </span>上带来一系列的技术<span lang="EN-US"> Demo</span>、<span lang="EN-US">Presentation</span>、<span lang="EN-US">PartyTime </span>等活动。据机器之心了解，商汤科技及香港中大<span lang="EN-US">-</span>商汤科技联合实验室共有<span lang="EN-US"> 23 </span>篇论文被接收。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">其中，商汤科技高级研究员钱晨的论文获得<span lang="EN-US"> CVPR2017<br>Spotlight </span>提名，他将在大会上做四分钟的演讲；商汤科技执行研发总监林倞也将会在<span lang="EN-US"> NITRE WORKSHOP<br></span>环节做出演讲。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">两篇<span lang="EN-US"> Presentation </span>论文分别为：</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.0pt;
font-family:Symbol;color:#3E3E3E">·<span style="font:7.0pt &quot;Times New Roman&quot;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br></span></span><span lang="EN-US" style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">Residual Attention Network for Image Classification</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.0pt;
font-family:Symbol;color:#3E3E3E">·<span style="font:7.0pt &quot;Times New Roman&quot;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br></span></span><span lang="EN-US" style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">Attention-aware Face Hallucination via Deep Reinforcement<br>Learning</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">日前机器之心对商汤科技<span lang="EN-US"> CVPR 2017 </span>论文的报道中，对其中的几篇论文进行了详细的解读，感兴趣的读者请参阅：<span lang="EN-US"><a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650729155&amp;idx=3&amp;sn=0450413d8851803ba0e0c35422c724b4&amp;chksm=871b2ebdb06ca7ab3256ea3b3a792f588b8c2f092d7387714742c6736d9d672a7d72c50c8360&amp;scene=21#wechat_redirect" target="_blank" style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="color:#607FA6"><span lang="EN-US">业界 | 23 </span></span><span lang="EN-US" style="color:#607FA6"><span lang="EN-US">篇<span lang="EN-US">论文入选 CVPR2017</span></span></span><span lang="EN-US" style="color:#607FA6"><span lang="EN-US">，商汤科技精选论文解读</span></span></a></span>。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p align="center" style="margin:0cm;margin-bottom:.0001pt;text-align:center;
line-height:21.0pt;background:white;max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important;min-height: 1em;orphans: auto;widows: 1;
-webkit-text-stroke-width: 0px;word-spacing:0px"><span style="max-width:100%;
box-sizing: border-box !important;word-wrap: break-word !important"><strong style="max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important"><span style="font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">学术界<span lang="EN-US"> CVPR 2017 </span>论文</span></strong></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">在这一部分，我们根据公开信息对学术界的论文进行了盘点。需要说明的是这些论文可能包含学术界与产业界合作的论文，而且以下某些院校被<span lang="EN-US"> CVPR 2017 </span>接收的论文可能远远超过我们收集到的，因此如果读者们发现我们遗漏了哪些精彩的论文，希望能够在下面留言。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">1. </span></span></strong><strong><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">厦门大学信息学院</span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文一：<span lang="EN-US">Cross-Modality Binary Code Learning via Fusion Similarity Hashing&nbsp;</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Hong Liu, Rongrong Ji</span>（纪荣嵘）<span lang="EN-US">, Yongjian Wu, Feiyue Huang, Baochang Zhang</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文二：<span lang="EN-US">Non-Local Deep Features<br>for Salient Object Detection</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li</span>（李绍滋）<span lang="EN-US">,<br>Pierre-Marc Jodoin</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文三：<span lang="EN-US">Re-Ranking Person Re-Identification With k-Reciprocal Encoding&nbsp;</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Zhun Zhong, Liang Zheng,<br>Donglin Cao, Shaozi Li</span>（李绍滋）</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文四：<span lang="EN-US">Removing Rain From Single Images via a Deep Detail Network&nbsp;</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao Ding</span>（丁兴号）<span lang="EN-US">, John Paisley&nbsp;</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px" id="PriSDL"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">2. </span></span></strong><strong><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">中国科学院大学电子学院模式识别与智能系统开发实验室</span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;word-spacing:0px;background: yellow;"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文一：<span lang="EN-US" style="
    /* background: yellow; */
">SRN: Side-output Residual Network for Object Symmetry Detection in the Wild</span>（<span lang="EN-US">CVPR 2017 Oral 1-1C)&nbsp;</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background: yellow;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;word-spacing:0px;"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：柯炜</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：论文创新性地提出了侧输出残差网络并将其成功应用在大规模复杂背景下的目标对称性检测任务中，取得了<span lang="EN-US"> State-Of-The-Art </span>性能。该架构显著地提升了多尺度特征融合性能，对尺度相关的深度学习任务具有应用价值。</span></p><img alt="SRN" src="/images/news/2017-7-23_4.jpg" style="
    max-width: 100%;
"><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background: yellow;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;word-spacing:0px;"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文二：<span lang="EN-US">Oriented Response Networks (CVPR 2017 Poster)</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background: yellow;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;word-spacing:0px;"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：周彦钊</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：论文从深度卷积网络最为核心的卷积模块出发，创新的设计了具有主动旋转能力的向量场滤波器<span lang="EN-US"><br>ARF</span>，赋予了深度网络显式编码方向信息的能力，并显著提升深度特征对旋转的泛化性能。该深度网络架构在多个评测中取得<span lang="EN-US"> State-Of-The-Art </span>性能。</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><img alt="ORN" src="/images/news/2017-7-23_5.jpg" style="
    max-width: 100%;
"><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background: yellow;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;word-spacing:0px;"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文三<span lang="EN-US">:Self-learning Scene-specific Pedestrian Detectors using a Progressive Latent Model (CVPR 2017 Poster)</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background: yellow;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;word-spacing:0px;"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：叶齐祥</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">简介：论文提出了一种新颖的针对特定场景的自学习行人检测算法，算法利用原始视频数据即可自动挖掘其中的行人目标并学习检测器，无需进行耗时耗力的逐帧数据标注，取得了<span lang="EN-US"> State-Of-The-Art </span>行人检测性能。</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><img alt="Weakly" src="/images/news/2017-7-23_6.jpg" style="
    max-width: 100%;
"><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">3. </span></span></strong><strong><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">南开大学媒体计算实验室</span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">&nbsp;</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文一：<span lang="EN-US">Richer Convolutional Features for Edge Detection</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Y Liu, MM Cheng, X Hu, K Wang, X Bai</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文二：<span lang="EN-US">Deeply supervised salient object detection with short connections</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Q Hou, MM Cheng, X Hu, Z<br>Tu, A Borji</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文三：<span lang="EN-US">GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">JW Bian, W Lin, Y<br>Matsushita, SK Yeung, TD Nguyen, MM Cheng</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">论文四：<span lang="EN-US">Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Y Wei, J Feng, X Liang, MM Cheng, Y Zhao, S Yan</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">4. </span></span></strong><strong><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">中科院计算所</span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">Learning Multifunctional Binary Codes for Both Category and Attribute Oriented Retrieval Tasks&nbsp;</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Haomiao Liu, Ruiping Wang</span>（王瑞平）<span lang="EN-US">, Shiguang Shan, Xilin Chen</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">5. </span></span></strong><strong><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">清华大学计算机系智能技术与系统国家重点实验室、清华国家信息实验室、清华大学计算机科学与技术系、英特尔中国研究院、清华大学电子工程系</span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E"><br style="max-width:100%;
box-sizing: border-box !important;word-wrap: break-word !important"><br></span><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E"></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">RON: Reverse Connection With Objectness Prior Networks for Object Detection&nbsp;</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：孔涛、孙富春、<span lang="EN-US">Anbang Yao</span>、刘华平、<span lang="EN-US">Ming Lu </span>和陈玉荣</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">Real-Time Neural Style Transfer for Videos</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, Wei Liu</span>（刘威，腾讯<span lang="EN-US"> AI</span>）</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">6. </span></span></strong><strong><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">大连理工大学信息与通信工程学院</span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">李培华两篇论文被<span lang="EN-US"> CVPR 2017 </span>录用<span lang="EN-US">, </span>其中一篇为<span lang="EN-US">**</span>口头报告。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">G2DeNet: Global Gaussian<br>Distribution Embedding Network and Its Application to Visual Recognition</span><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">（<span lang="EN-US">Oral 4-2A</span>）</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Qilong Wang, Peihua Li</span>（李培华）<span lang="EN-US">, Lei Zhang</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">&nbsp;Mind the Class Weight Bias: Weighted Maximum Mean Discrepancy for Unsupervised Domain Adaptation</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Hongliang Yan, Yukang Ding, Peihua Li</span>（李培华）<span lang="EN-US">, Qilong Wang, Yong Xu, Wangmeng Zuo</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">7. </span></span></strong><strong><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">北京大学计算机科学技术研究所字形计算技术实验室</span></strong></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">北大学生刘俊成与大连理工大学、辽宁省泛在网络与服务软件重点实验室合作的一篇论文。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:19.2pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">Incremental Kernel Null Space Discriminant Analysis for Novelty Detection</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">作者：<span lang="EN-US">Juncheng Liu</span>（刘俊成）<span lang="EN-US">, Zhouhui Lian, Yi Wang, Jianguo Xiao</span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p align="center" style="margin:0cm;margin-bottom:.0001pt;text-align:center;
line-height:21.0pt;background:white;max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important;min-height: 1em;orphans: auto;widows: 1;
-webkit-text-stroke-width: 0px;word-spacing:0px"><span style="max-width:100%;
box-sizing: border-box !important;word-wrap: break-word !important"><strong style="max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important"><span style="font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">更多亮点</span></strong></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-align:justify;text-justify:
inter-ideograph;text-indent:21.0pt;line-height:21.0pt;background:white;
max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">多家海外科技巨头也在<span lang="EN-US"> CVPR </span>开幕前夕介绍了自己在大会上即将展示的研究。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">谷歌</span></span></strong><span style="max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important"><span class="apple-converted-space"><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E"></span></span><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">CVPR 2017 </span><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">研究集合：<span lang="EN-US"><a href="https://research.googleblog.com/2017/07/google-at-cvpr-2017.html" target="_blank" rel="noopener">https://research.googleblog.com/2017/07/google-at-cvpr-2017.html</a></span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="font-size:10.5pt;font-family:
&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">共有超过<span lang="EN-US"> 250 </span>名<span lang="EN-US"> Google </span>员工将参加本次会议，同时参与和组织<span lang="EN-US"> CVPR </span>上的多个研讨会。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">Facebook</span></span></strong><span style="max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important"><span class="apple-converted-space"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E"></span></span><span lang="EN-US" style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">CVPR 2017 </span><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">研究集合：<span lang="EN-US"><a href="https://research.fb.com/advancing-computer-vision-technologies-at-cvpr-2017/" target="_blank" rel="noopener">https://research.fb.com/advancing-computer-vision-technologies-at-cvpr-2017/</a></span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">Facebook </span><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">与<span lang="EN-US"> MIT Media Lab </span>的研究《<span lang="EN-US">Robocodes: Towards Generative Street Addresses from Satellite Imagery</span>》<strong style="max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important"><span style="font-family:&quot;微软雅黑&quot;,sans-serif">获得了大会<span lang="EN-US"> workshop </span>最佳论文奖</span></strong>，在<span lang="EN-US"> EarthVision </span>研讨上，研究人员将介绍这项工作，它有关遥感图像的大规模计算机视觉。</span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:19.2pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;text-align:start;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:21.0pt;line-height:21.0pt;
background:white;max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important;
min-height: 1em;orphans: auto;widows: 1;-webkit-text-stroke-width: 0px;
word-spacing:0px"><strong style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span style="max-width:100%;box-sizing: border-box !important;
word-wrap: break-word !important"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">IBM</span></span></strong><span style="max-width:100%;box-sizing: border-box !important;word-wrap: break-word !important"><span class="apple-converted-space"><span lang="EN-US" style="font-size:10.5pt;
font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E"></span></span><span lang="EN-US" style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">CVPR 2017 </span><span style="font-size:10.5pt;font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">研究集合：<span lang="EN-US"><a href="https://www.ibm.com/blogs/research/2017/07/computer-vision-cvpr-2017/" target="_blank" rel="noopener">https://www.ibm.com/blogs/research/2017/07/computer-vision-cvpr-2017/</a></span></span></span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white"><span lang="EN-US" style="font-family:&quot;微软雅黑&quot;,sans-serif;
color:#3E3E3E">&nbsp;</span></p><p style="margin:0cm;margin-bottom:.0001pt;text-indent:24.0pt;line-height:21.0pt;
background:white"><span style="font-family:&quot;微软雅黑&quot;,sans-serif;color:#3E3E3E">本条新闻来自《机器之心》报道：<a href="http://mp.weixin.qq.com/s/0ypB-tkiGUd5ZZyDb69o0Q" target="_blank" rel="noopener">CVPR 2017 国内外亮点论文汇集：史上最盛大会议，华人占据半壁江山</a></span></p></div><p></p></td></tr></tbody></table>
        </div>
        <!-- <footer class="article-footer">
            
    <div class="jiathis_style">
    <a class="jiathis_button_copy">复制链接</a>
    <a class="jiathis_button_email">邮件</a>
    <a class="jiathis_button_weixin">微信</a>
    <a class="jiathis_button_cqq">QQ好友</a>
    <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
    <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<style>
    .jiathis_style {
        float: right;
    }
    .jiathis_style div:first-child:not(.jiadiv_01) {
        width: auto !important;
        border: none !important;
    }
    .jiathis_style .jiadiv_01 {
        margin: 10px 0;
        border-radius: 4px;
        border: #e1e1e1 solid 1px;
    }
    .jiathis_style .jiadiv_01 div:first-child {
        display: none;
    }
    .jiathis_style .jiadiv_02 {
        padding: 7px 0 !important;
    }
    .jiathis_style .jiadiv_02 .jiatitle {
        width: 85px;
        border: none;
        height: auto;
        margin: 3px 10px;
        padding: 6px 10px;
        border-radius: 4px;
    }
    .jiathis_style .jiadiv_02 .jiatitle:hover {
        border: none;
    }
    .jiathis_style .jiadiv_02 .jiatitle:nth-child(even) {
        margin-left: 0;
    }
    .jiathis_style .jtico:hover {
        opacity: 1;
    }
    .jiathis_style .ckepopBottom,
    .jiathis_style .centerBottom {
        width: auto !important;
        padding: 5px;
        background: #f7f7f7;
    }
</style>




        </footer> -->
    </div>
</article>


                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>联系我们 :</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="地理位置" href="http://j.map.baidu.com/wQNCN" target="_blank">
                        <i class="icon fa fa-map-marker"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="发送邮件" href="mailto:PriSDL@outlook.com" target="_blank">
                        <i class="icon fa fa-envelope"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/content/news/2017-9-1.html" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">下一篇</strong>
        <p class="article-nav-title">
        
            杜克大学卫奇博士到智能系统开发实验室做学术报告
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/content/news/2017-7-18.html" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">上一篇</strong>
        <p class="article-nav-title">实验室一篇论文被国际顶级会议ICCV2017录用</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                
    <div class="widget-wrap">
        <h3 class="widget-title">最新动态</h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/content/news/2018-3-18.html" class="thumbnail">
    
    
        <span style="background-image:url(/images/news/2018-3-18_0.jpg)" alt="祝贺柯炜同学通过博士论文答辩" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/新闻/">新闻</a></p>
                            <p class="item-title"><a href="/content/news/2018-3-18.html" class="title">祝贺柯炜同学通过博士论文答辩</a></p>
                            <p class="item-date"><time datetime="2018-03-17T16:00:00.000Z" itemprop="datePublished">2018-03-18</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/content/news/2018-3-1.html" class="thumbnail">
    
    
        <span style="background-image:url(/images/news/2018-3-1_1.jpg)" alt="实验室三篇论文被国际顶级会议CVPR2018录用" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/新闻/">新闻</a></p>
                            <p class="item-title"><a href="/content/news/2018-3-1.html" class="title">实验室三篇论文被国际顶级会议CVPR2018录用</a></p>
                            <p class="item-date"><time datetime="2018-02-28T16:00:00.000Z" itemprop="datePublished">2018-03-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/content/news/2017-11-15.html" class="thumbnail">
    
    
        <span style="background-image:url(/images/news/2017-11-15_0.png)" alt="实验室在ICCV’17 Workshop对称性检测比赛中取得好成绩" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/新闻/">新闻</a></p>
                            <p class="item-title"><a href="/content/news/2017-11-15.html" class="title">实验室在ICCV’17 Workshop对称性检测比赛中取得好成绩</a></p>
                            <p class="item-date"><time datetime="2017-11-14T16:00:00.000Z" itemprop="datePublished">2017-11-15</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/content/news/2017-11-1.html" class="thumbnail">
    
    
        <span style="background-image:url(/images/news/2017-11-1_0.jpg)" alt="实验室三名同学参加IEEE ICCV2017国际会议" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/新闻/">新闻</a></p>
                            <p class="item-title"><a href="/content/news/2017-11-1.html" class="title">实验室三名同学参加IEEE ICCV2017国际会议</a></p>
                            <p class="item-date"><time datetime="2017-10-31T16:00:00.000Z" itemprop="datePublished">2017-11-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/content/news/2017-9-30.html" class="thumbnail">
    
    
        <span style="background-image:url(/images/news/2017-9-30_0.jpg)" alt="【传媒聚焦】泰伯网对实验室遥感竞赛获奖团队专题报道" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/新闻/">新闻</a></p>
                            <p class="item-title"><a href="/content/news/2017-9-30.html" class="title">【传媒聚焦】泰伯网对实验室遥感竞赛获奖团队专题报道</a></p>
                            <p class="item-date"><time datetime="2017-09-29T16:00:00.000Z" itemprop="datePublished">2017-09-30</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">链接</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://www.ucas.ac.cn">中国科学院大学</a>
                    </li>
                
                    <li>
                        <a href="http://eece.ucas.ac.cn">电子电气与通信工程学院</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
</aside>
                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="http://ucassdl.cn" class="logo"></a>
                </h1>
                <p>&copy; 2018 中国科学院大学·模式识别与智能系统开发实验室</p>
            </div>
        </div>
    </div>
</footer>
        


    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>
